#!/usr/bin/env python3
"""
R-2 Swallowæ¨è«–åŠ¹ç‡æ¸¬å®š ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å®Ÿè£…å®Œäº†ç¢ºèªã¨ãƒ­ãƒ¼ã‚«ãƒ«å‹•ä½œãƒ†ã‚¹ãƒˆç”¨
"""

import sys
import json
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

try:
    from Python.Benchmark.swallow_inference_benchmark import SwallowInferenceBenchmark
    print("âœ… SwallowInferenceBenchmark import successful")
except ImportError as e:
    print(f"âŒ Import failed: {e}")
    sys.exit(1)

def test_prompt_loading():
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
    prompt_file = "dataset/prompts_swallow_bench.jsonl"
    
    if not Path(prompt_file).exists():
        print(f"âŒ Prompt file not found: {prompt_file}")
        return False
    
    try:
        with open(prompt_file, 'r', encoding='utf-8') as f:
            prompts = [json.loads(line.strip()) for line in f]
        
        print(f"âœ… Loaded {len(prompts)} prompts from {prompt_file}")
        print(f"   First prompt: {prompts[0]['prompt'][:50]}...")
        return True
    
    except Exception as e:
        print(f"âŒ Prompt loading failed: {e}")
        return False

def test_benchmark_initialization():
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¯ãƒ©ã‚¹åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
    try:
        benchmark = SwallowInferenceBenchmark(results_dir="test_results")
        print("âœ… SwallowInferenceBenchmark initialized successfully")
        
        # å¿…è¦ãƒ¡ã‚½ãƒƒãƒ‰ã®å­˜åœ¨ç¢ºèª
        required_methods = [
            'load_prompts',
            'bootstrap_confidence_interval', 
            'run_comparative_benchmark',
            'print_summary'
        ]
        
        for method in required_methods:
            if hasattr(benchmark, method):
                print(f"   âœ… Method {method} found")
            else:
                print(f"   âŒ Method {method} missing")
                return False
        
        return True
    
    except Exception as e:
        print(f"âŒ Benchmark initialization failed: {e}")
        return False

def test_validation_runner_integration():
    """æ¤œè¨¼ãƒ©ãƒ³ãƒŠãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆ"""
    try:
        from Python.Validation.paper_validation_runner import PaperValidationRunner
        
        runner = PaperValidationRunner(output_dir="test_validation_results")
        print("âœ… PaperValidationRunner initialized successfully")
        
        # R-2æ¤œè¨¼ãƒ¡ã‚½ãƒƒãƒ‰ã®å­˜åœ¨ç¢ºèª
        if hasattr(runner, 'validate_r2_swallow_efficiency'):
            print("   âœ… validate_r2_swallow_efficiency method found")
            return True
        else:
            print("   âŒ validate_r2_swallow_efficiency method missing")
            return False
    
    except Exception as e:
        print(f"âŒ ValidationRunner integration failed: {e}")
        return False

def test_mock_benchmark_run():
    """ãƒ¢ãƒƒã‚¯ç’°å¢ƒã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œãƒ†ã‚¹ãƒˆ"""
    try:
        benchmark = SwallowInferenceBenchmark(results_dir="test_results")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿
        prompt_file = "dataset/prompts_swallow_bench.jsonl"
        if not Path(prompt_file).exists():
            print("âŒ Skipping mock benchmark test - prompts not found")
            return False
        
        prompts = benchmark.load_prompts(prompt_file)
        print(f"âœ… Mock test: Loaded {len(prompts)} prompts")
        
        # ä¿¡é ¼åŒºé–“è¨ˆç®—ãƒ†ã‚¹ãƒˆ
        test_data = [1.0, 1.2, 0.9, 1.1, 1.05]
        ci = benchmark.bootstrap_confidence_interval(test_data)
        print(f"âœ… Mock test: Confidence interval calculation works: {ci}")
        
        return True
    
    except Exception as e:
        print(f"âŒ Mock benchmark test failed: {e}")
        return False

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ¡ã‚¤ãƒ³"""
    print("="*50)
    print("R-2 Swallow Inference Efficiency Implementation Test")
    print("="*50)
    
    tests = [
        ("Prompt Loading", test_prompt_loading),
        ("Benchmark Initialization", test_benchmark_initialization),
        ("ValidationRunner Integration", test_validation_runner_integration),
        ("Mock Benchmark Run", test_mock_benchmark_run)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ§ª Testing: {test_name}")
        if test_func():
            passed += 1
        else:
            print(f"   Test failed: {test_name}")
    
    print(f"\n" + "="*50)
    print(f"Test Results: {passed}/{total} passed")
    
    if passed == total:
        print("ğŸ‰ All tests passed! R-2 implementation ready.")
    else:
        print("âš ï¸  Some tests failed. Check implementation.")
    
    print("="*50)

if __name__ == "__main__":
    main()
