#!/usr/bin/env python3
"""
Scientific Framework Launch Script
ç§‘å­¦çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯çµ±åˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Author: Akira Ito a.k.a limonene213u
çµ±åˆå®Ÿè¡Œ: å³åº§å®Ÿè£…å¯èƒ½ãªæœ€é©åŒ–ã‹ã‚‰å®Œå…¨ãªç§‘å­¦çš„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¾ã§
"""

import os
import sys
import argparse
import asyncio
import logging
from pathlib import Path
from typing import Optional, List
import json

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
try:
    from scientific_optimization_framework import JapaneseSpecializedModel, MI300XConfig, OptimizationLevel
    OPTIMIZATION_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Optimization framework not available: {e}")
    OPTIMIZATION_AVAILABLE = False

try:
    from vaporetto_integration import DeepSeekVaporettoIntegration
    VAPORETTO_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Vaporetto integration not available: {e}")
    VAPORETTO_AVAILABLE = False

try:
    from jlce_evaluation_system import JLCEEvaluator
    JLCE_AVAILABLE = True
except ImportError as e:
    print(f"Warning: JLCE evaluation not available: {e}")
    JLCE_AVAILABLE = False

try:
    from scientific_japanese_adaptation_pipeline import ScientificJapaneseAdaptationPipeline
    PIPELINE_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Full pipeline not available: {e}")
    PIPELINE_AVAILABLE = False

MODULES_AVAILABLE = any([OPTIMIZATION_AVAILABLE, VAPORETTO_AVAILABLE, JLCE_AVAILABLE, PIPELINE_AVAILABLE])

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FrameworkLauncher:
    """ç§‘å­¦çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯çµ±åˆãƒ©ãƒ³ãƒãƒ£ãƒ¼"""
    
    def __init__(self):
        self.available_models = [
            "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
            "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", 
            "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
            "deepseek-ai/deepseek-r1-distill-qwen-1.5b"
        ]
        
        self.execution_modes = {
            "quick": "å³åº§å®Ÿè£…æœ€é©åŒ–ï¼ˆ5-10åˆ†ï¼‰",
            "analysis": "åˆ†æãƒ»è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ15-30åˆ†ï¼‰", 
            "full": "å®Œå…¨ç§‘å­¦çš„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆ60-120åˆ†ï¼‰",
            "benchmark": "ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»æ¯”è¼ƒå®Ÿè¡Œï¼ˆ30-60åˆ†ï¼‰"
        }
    
    def show_menu(self):
        """ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¡¨ç¤º"""
        print("\n" + "="*70)
        print("DEEPSEEK R1 SCIENTIFIC OPTIMIZATION FRAMEWORK")
        print("="*70)
        print("ç§‘å­¦çš„æœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ - çµ±åˆå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ")
        print()
        
        print("å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰:")
        for mode, description in self.execution_modes.items():
            print(f"  {mode}: {description}")
        print()
        
        print("å¯¾å¿œãƒ¢ãƒ‡ãƒ«:")
        for i, model in enumerate(self.available_models, 1):
            model_name = model.split('/')[-1]
            print(f"  {i}. {model_name}")
        print()
    
    def get_user_selection(self) -> tuple:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠå–å¾—"""
        # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ
        while True:
            mode = input("å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ (quick/analysis/full/benchmark): ").strip().lower()
            if mode in self.execution_modes:
                break
            print("ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚quick, analysis, full, benchmark ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        while True:
            try:
                print("\nãƒ¢ãƒ‡ãƒ«é¸æŠ:")
                for i, model in enumerate(self.available_models, 1):
                    print(f"  {i}. {model.split('/')[-1]}")
                
                choice = input("ãƒ¢ãƒ‡ãƒ«ç•ªå·ã‚’é¸æŠ (1-4): ").strip()
                model_index = int(choice) - 1
                
                if 0 <= model_index < len(self.available_models):
                    selected_model = self.available_models[model_index]
                    break
                else:
                    print("ç„¡åŠ¹ãªç•ªå·ã§ã™ã€‚1-4ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ã€‚")
            except ValueError:
                print("æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
        return mode, selected_model
    
    async def run_quick_optimization(self, model_name: str):
        """å³åº§å®Ÿè£…æœ€é©åŒ–å®Ÿè¡Œ"""
        print(f"\nğŸš€ å³åº§å®Ÿè£…æœ€é©åŒ–ã‚’é–‹å§‹: {model_name}")
        
        if not OPTIMIZATION_AVAILABLE:
            print("âŒ æœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            print("ğŸ“ åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®ã¿è¡¨ç¤º:")
            print(f"  é¸æŠãƒ¢ãƒ‡ãƒ«: {model_name}")
            print(f"  Pythonç‰ˆ: {sys.version}")
            print(f"  åˆ©ç”¨å¯èƒ½ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: optimization={OPTIMIZATION_AVAILABLE}, vaporetto={VAPORETTO_AVAILABLE}")
            return
        
        try:
            # MI300Xæœ€é©åŒ–è¨­å®š
            config = MI300XConfig(optimization_level=OptimizationLevel.ADVANCED)
            
            # æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            japanese_model = JapaneseSpecializedModel(model_name, config)
            
            # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º
            status = japanese_model.get_system_status()
            print("\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹:")
            print(f"  ãƒ¢ãƒ‡ãƒ«: {status['model_name']}")
            print(f"  æœ€é©åŒ–ãƒ¬ãƒ™ãƒ«: {status['mi300x_config']['optimization_level']}")
            print(f"  ROCmç’°å¢ƒ: {status['rocm_environment'].get('torch_rocm_support', 'Unknown')}")
            
            # Vaporettoçµ±åˆãƒ†ã‚¹ãƒˆ
            if VAPORETTO_AVAILABLE:
                print("\nâš¡ Vaporettoçµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
                vaporetto = DeepSeekVaporettoIntegration(model_name)
                
                test_texts = [
                    "æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æ—¥æœ¬èªå‡¦ç†ã®æœ€é©åŒ–",
                    "DeepSeek R1ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡"
                ]
                
                comparison = vaporetto.compare_tokenization_efficiency(test_texts)
                
                print("ğŸ“ˆ æœ€é©åŒ–çµæœ:")
                print(f"  å‡¦ç†é€Ÿåº¦å‘ä¸Š: {comparison.get('speed_improvement', 'N/A'):.2f}x")
                print(f"  åŠ¹ç‡æ¯”ç‡: {comparison.get('efficiency_ratio', 'N/A'):.2f}")
            else:
                print("âš ï¸  Vaporettoçµ±åˆã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æœªåˆ©ç”¨å¯èƒ½ï¼‰")
            
            print("\nâœ… å³åº§å®Ÿè£…æœ€é©åŒ–å®Œäº†!")
            
        except Exception as e:
            print(f"âŒ æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"Quick optimization failed: {e}")
    
    async def run_analysis_system(self, model_name: str):
        """åˆ†æãƒ»è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ"""
        print(f"\nğŸ”¬ åˆ†æãƒ»è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹: {model_name}")
        
        if not JLCE_AVAILABLE:
            print("âŒ JLCEè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            print("ğŸ“ åŸºæœ¬åˆ†æã®ã¿å®Ÿè¡Œ:")
            
            # åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ åˆ†æ
            if OPTIMIZATION_AVAILABLE:
                try:
                    config = MI300XConfig()
                    japanese_model = JapaneseSpecializedModel(model_name, config)
                    status = japanese_model.get_system_status()
                    
                    print("\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹:")
                    print(f"  ãƒ¢ãƒ‡ãƒ«: {status['model_name']}")
                    print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {status.get('memory_usage', 'Unknown')}")
                    print(f"  æœ€é©åŒ–çŠ¶æ…‹: {status.get('optimization_status', 'Unknown')}")
                except Exception as e:
                    print(f"âš ï¸  åŸºæœ¬åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            
            if VAPORETTO_AVAILABLE:
                try:
                    print("\nâš¡ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–åŠ¹ç‡ãƒ†ã‚¹ãƒˆ:")
                    vaporetto = DeepSeekVaporettoIntegration(model_name)
                    test_texts = ["ç§‘å­¦çš„æœ€é©åŒ–ã®è©•ä¾¡ã¨åˆ†æ", "æ—¥æœ¬èªè¨€èªãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¸¬å®š"]
                    comparison = vaporetto.compare_tokenization_efficiency(test_texts)
                    print(f"  é€Ÿåº¦å‘ä¸Š: {comparison.get('speed_improvement', 'N/A'):.2f}x")
                except Exception as e:
                    print(f"âš ï¸  ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            
            print("\nâœ… åŸºæœ¬åˆ†æå®Œäº†!")
            return
        
        try:
            # JLCEè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
            evaluator = JLCEEvaluator()
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡ãƒ†ã‚¹ãƒˆ
            from jlce_evaluation_system import create_sample_test_data
            test_datasets = create_sample_test_data()
            
            print(f"ğŸ“ è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°: {len(test_datasets)}")
            print("ğŸ“Š è©•ä¾¡ã‚¿ã‚¹ã‚¯:")
            for task_name in test_datasets.keys():
                print(f"  - {task_name}")
            
            # çµ±åˆåˆ†æå®Ÿè¡Œ
            if VAPORETTO_AVAILABLE:
                vaporetto = DeepSeekVaporettoIntegration(model_name)
                
                test_texts = [
                    "æ—¥æœ¬èªã®è‡ªç„¶è¨€èªå‡¦ç†ã«ãŠã‘ã‚‹èª²é¡Œ",
                    "æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨æœ€é©åŒ–æ‰‹æ³•",
                    "æ–‡åŒ–çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸè¨€èªç†è§£"
                ]
                
                # æ—¥æœ¬èªç‰¹æ€§åˆ†æ
                characteristics = vaporetto.analyze_japanese_characteristics(test_texts)
                
                print("\nğŸ“ˆ æ—¥æœ¬èªç‰¹æ€§åˆ†æçµæœ:")
                char_dist = characteristics["character_distribution"]
                for script, stats in char_dist.items():
                    print(f"  {script}: {stats['mean']:.3f} Â± {stats['std']:.3f}")
            
            print("\nâœ… åˆ†æãƒ»è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ å®Œäº†!")
            
        except Exception as e:
            print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"Analysis system failed: {e}")
    
    async def run_full_pipeline(self, model_name: str):
        """å®Œå…¨ç§‘å­¦çš„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print(f"\nğŸ§ª å®Œå…¨ç§‘å­¦çš„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹å§‹: {model_name}")
        
        if not MODULES_AVAILABLE:
            print("Error: Required modules not available")
            return
        
        try:
            # ç§‘å­¦çš„æ—¥æœ¬èªç‰¹åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
            pipeline = ScientificJapaneseAdaptationPipeline(
                model_name=model_name,
                output_dir="scientific_pipeline_results",
                enable_experimental=False
            )
            
            print("ğŸ”„ æœ€é©åŒ–ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œä¸­...")
            print("  Stage 1: åˆæœŸè§£æ (5åˆ†)")
            print("  Stage 2: æ·±å±¤è§£æ (15åˆ†)")
            print("  Stage 3: æˆ¦ç•¥ç­–å®š (10åˆ†)")
            print("  Stage 4: å®Ÿè£…ãƒ»è©•ä¾¡ (ç¶™ç¶š)")
            
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
            report = await pipeline.execute_optimization_cycle()
            
            print("\nğŸ“Š ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœ:")
            print(f"  Pipeline ID: {report.pipeline_id}")
            print(f"  ç·å®Ÿè¡Œæ™‚é–“: {report.total_duration:.2f}ç§’")
            print(f"  æˆåŠŸç‡: {report.overall_success_rate:.2%}")
            
            print("\nğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„:")
            if report.adaptation_strategy:
                for metric, value in report.adaptation_strategy.expected_improvements.items():
                    print(f"  {metric}: {value:.1f}x")
            
            print(f"\nğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {pipeline.output_dir}")
            print("\nâœ… å®Œå…¨ç§‘å­¦çš„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")
            
        except Exception as e:
            print(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def run_benchmark(self, model_name: str):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»æ¯”è¼ƒå®Ÿè¡Œ"""
        print(f"\nâ±ï¸ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»æ¯”è¼ƒã‚’é–‹å§‹: {model_name}")
        
        try:
            # è¤‡æ•°ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            print("ğŸ”„ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
            
            results = {
                "model": model_name,
                "timestamp": pd.Timestamp.now().isoformat(),
                "benchmarks": {}
            }
            
            if MODULES_AVAILABLE:
                # Vaporettoæ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
                vaporetto = DeepSeekVaporettoIntegration(model_name)
                
                test_texts = [
                    "æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹è‡ªç„¶è¨€èªå‡¦ç†ã®ç ”ç©¶",
                    "æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ—¥æœ¬èªå¯¾å¿œæœ€é©åŒ–",
                    "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ€§èƒ½è©•ä¾¡",
                    "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è§£æ",
                    "è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ–‡åŒ–çš„é©å¿œæ€§å‘ä¸Š"
                ]
                
                comparison = vaporetto.compare_tokenization_efficiency(test_texts)
                results["benchmarks"]["tokenization"] = comparison
                
                print("ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½:")
                print(f"  Vaporettoé€Ÿåº¦: {comparison['vaporetto']['tokens_per_second']:.2f} tokens/sec")
                print(f"  DeepSeeké€Ÿåº¦: {comparison['deepseek']['tokens_per_second']:.2f} tokens/sec")
                print(f"  æ”¹å–„ç‡: {comparison.get('efficiency_ratio', 1):.2f}x")
            
            # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
            if MODULES_AVAILABLE:
                config = MI300XConfig()
                japanese_model = JapaneseSpecializedModel(model_name, config)
                system_status = japanese_model.get_system_status()
                results["system_status"] = system_status
            
            # çµæœä¿å­˜
            output_file = f"benchmark_results_{int(time.time())}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"\nğŸ“ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœä¿å­˜: {output_file}")
            print("\nâœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»æ¯”è¼ƒå®Œäº†!")
            
        except Exception as e:
            print(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="DeepSeek R1 Scientific Optimization Framework")
    parser.add_argument("--mode", choices=["quick", "analysis", "full", "benchmark"], 
                       help="å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--model", type=int, choices=[1, 2, 3, 4], 
                       help="ãƒ¢ãƒ‡ãƒ«é¸æŠ (1-4)")
    parser.add_argument("--interactive", action="store_true", 
                       help="å¯¾è©±å½¢å¼ã§å®Ÿè¡Œ")
    
    args = parser.parse_args()
    
    launcher = FrameworkLauncher()
    
    if args.interactive or (not args.mode or not args.model):
        # å¯¾è©±ãƒ¢ãƒ¼ãƒ‰
        launcher.show_menu()
        mode, model_name = launcher.get_user_selection()
    else:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãƒ¢ãƒ¼ãƒ‰
        mode = args.mode
        model_name = launcher.available_models[args.model - 1]
    
    print(f"\nğŸ¯ å®Ÿè¡Œè¨­å®š:")
    print(f"  ãƒ¢ãƒ¼ãƒ‰: {mode} - {launcher.execution_modes[mode]}")
    print(f"  ãƒ¢ãƒ‡ãƒ«: {model_name.split('/')[-1]}")
    
    # ç¢ºèª
    if not args.interactive:
        confirm = input("\nå®Ÿè¡Œã—ã¾ã™ã‹? (y/N): ").strip().lower()
        if confirm != 'y':
            print("å®Ÿè¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
            return
    
    # å®Ÿè¡Œ
    print(f"\nğŸš€ {launcher.execution_modes[mode]} ã‚’é–‹å§‹...")
    
    if mode == "quick":
        await launcher.run_quick_optimization(model_name)
    elif mode == "analysis":
        await launcher.run_analysis_system(model_name)
    elif mode == "full":
        await launcher.run_full_pipeline(model_name)
    elif mode == "benchmark":
        await launcher.run_benchmark(model_name)
    
    print("\nğŸ‰ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Ÿè¡Œå®Œäº†!")
    print("è©³ç´°çµæœã¯ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    import time
    import pandas as pd
    asyncio.run(main())
