#!/usr/bin/env python3
"""MLA vs standard attention benchmark.

This script measures the KV cache memory usage and related metrics of
DeepSeek R1 models employing Multi-Head Latent Attention (MLA) and
compares them to a baseline model that uses standard attention.  The
goal is to reproduce the paper's claim that MLA reduces KV cache size by
5--13% compared with a conventional attention mechanism.  The benchmark
is designed to run on ROCm-enabled GPUs but falls back to CPU execution
if no GPU is available.
"""

import torch
import torch.nn as nn
import time
import psutil
import gc
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, field
import json
import logging
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import numpy as np

@dataclass
class MLABenchmarkConfig:
    """MLAåŠ¹ç‡æ¸¬å®šè¨­å®š"""
    model_name: str
    baseline_model_name: str = "meta-llama/Llama-2-7b-hf"  # æ¨™æº–Attentionæ¯”è¼ƒç”¨
    sequence_lengths: List[int] = None
    batch_sizes: List[int] = None
    precision_modes: List[str] = None
    num_runs: int = 5
    warmup_runs: int = 2
    output_dir: str = "benchmark_results"
    
    def __post_init__(self):
        if self.sequence_lengths is None:
            self.sequence_lengths = [512, 1024, 2048, 4096]
        if self.batch_sizes is None:
            self.batch_sizes = [1, 2, 4, 8]
        if self.precision_modes is None:
            self.precision_modes = ["fp16", "bf16"]

@dataclass
class AttentionBenchmarkResult:
    """AttentionåŠ¹ç‡æ¸¬å®šçµæœ"""
    model_name: str
    attention_type: str
    sequence_length: int
    batch_size: int
    precision: str
    kv_cache_memory_mb: float
    attention_computation_time_ms: float
    peak_memory_usage_mb: float
    throughput_tokens_per_sec: float
    measurement_timestamp: str

class MLAEfficiencyMeasurer:
    """MLAåŠ¹ç‡æ¸¬å®šã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: MLABenchmarkConfig):
        self.config = config
        self.logger = self._setup_logger()
        self.results: List[AttentionBenchmarkResult] = []
        
        # GPU/ROCmç’°å¢ƒè¨­å®š
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        if self.device == "cuda":
            self.logger.info(f"GPU Device: {torch.cuda.get_device_name()}")
            self.logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚°è¨­å®š"""
        logger = logging.getLogger("MLA_Benchmark")
        logger.setLevel(logging.INFO)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©
        log_file = Path(self.config.output_dir) / "mla_benchmark.log"
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def load_model_with_precision(self, precision: str) -> Tuple[Any, Any]:
        """æŒ‡å®šç²¾åº¦ã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        self.logger.info(f"Loading model {self.config.model_name} with precision {precision}")
        
        tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
        
        # ç²¾åº¦è¨­å®š
        torch_dtype = torch.float16 if precision == "fp16" else torch.bfloat16
        
        model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            torch_dtype=torch_dtype,
            device_map="auto" if self.device == "cuda" else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        return model, tokenizer
    
    def generate_test_sequences(self, tokenizer: Any, seq_length: int, batch_size: int) -> torch.Tensor:
        """ãƒ†ã‚¹ãƒˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆ"""
        japanese_texts = [
            "æ—¥æœ¬èªã®è‡ªç„¶è¨€èªå‡¦ç†æŠ€è¡“ã¯è¿‘å¹´å¤§ããé€²æ­©ã—ã¦ãŠã‚Šã€ç‰¹ã«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç™ºå±•ã«ã‚ˆã‚Š",
            "æ©Ÿæ¢°å­¦ç¿’ã¨æ·±å±¤å­¦ç¿’ã®æŠ€è¡“é©æ–°ã«ã‚ˆã‚Šã€äººå·¥çŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ãŒé£›èºçš„ã«å‘ä¸Šã—ã¦ã„ã‚‹",
            "ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã¯è‡ªç„¶è¨€èªå‡¦ç†ã«ãŠã„ã¦é‡è¦ãªå‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚ã‚Šã€æ—¥æœ¬èªã®å ´åˆ",
            "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã¯ç¾ä»£ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã®ä¸­æ ¸æŠ€è¡“ã§ã‚ã‚Šã€æ–‡è„ˆç†è§£èƒ½åŠ›ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹",
            "ROCmãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¨AMD GPUã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€æ©Ÿæ¢°å­¦ç¿’ã®è¨ˆç®—åŠ¹ç‡ã‚’æœ€é©åŒ–ã§ãã‚‹"
        ]
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™
        texts = (japanese_texts * ((batch_size // len(japanese_texts)) + 1))[:batch_size]
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆæŒ‡å®šé•·ã•ã«èª¿æ•´ï¼‰
        encoded = tokenizer(
            texts,
            padding='max_length',
            truncation=True,
            max_length=seq_length,
            return_tensors="pt"
        )
        
        return encoded['input_ids'].to(self.device)
    
    def measure_kv_cache_memory(self, model: Any, input_ids: torch.Tensor) -> float:
        """KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š"""
        if self.device == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
        initial_memory = torch.cuda.memory_allocated() if self.device == "cuda" else 0
        
        with torch.no_grad():
            # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ã§ã®æ¨è«–
            _ = model(input_ids, use_cache=True)
            
        # ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        peak_memory = torch.cuda.max_memory_allocated() if self.device == "cuda" else 0
        kv_cache_memory = (peak_memory - initial_memory) / (1024 * 1024)  # MB
        
        return kv_cache_memory
    
    def measure_attention_computation_time(self, model: Any, input_ids: torch.Tensor) -> float:
        """Attentionè¨ˆç®—æ™‚é–“æ¸¬å®š"""
        model.eval()
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(self.config.warmup_runs):
            with torch.no_grad():
                _ = model(input_ids)
        
        # è¨ˆç®—æ™‚é–“æ¸¬å®š
        if self.device == "cuda":
            torch.cuda.synchronize()
            
        start_time = time.perf_counter()
        
        for _ in range(self.config.num_runs):
            with torch.no_grad():
                _ = model(input_ids)
                
        if self.device == "cuda":
            torch.cuda.synchronize()
            
        end_time = time.perf_counter()
        
        avg_time_ms = ((end_time - start_time) / self.config.num_runs) * 1000
        return avg_time_ms
    
    def measure_throughput(self, model: Any, tokenizer: Any, input_ids: torch.Tensor) -> float:
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®šï¼ˆtokens/secï¼‰"""
        model.eval()
        
        if self.device == "cuda":
            torch.cuda.synchronize()
            
        start_time = time.perf_counter()
        
        with torch.no_grad():
            # ç”Ÿæˆå®Ÿè¡Œ
            generated = model.generate(
                input_ids,
                max_new_tokens=50,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id
            )
            
        if self.device == "cuda":
            torch.cuda.synchronize()
            
        end_time = time.perf_counter()
        
        # ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
        new_tokens = generated.shape[1] - input_ids.shape[1]
        total_tokens = new_tokens * input_ids.shape[0]
        
        throughput = total_tokens / (end_time - start_time)
        return throughput
    
    def run_baseline_comparison(self, seq_length: int, batch_size: int, precision: str) -> Tuple[AttentionBenchmarkResult, AttentionBenchmarkResult]:
        """MLA vs æ¨™æº–Attention ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒï¼ˆè«–æ–‡è¨˜è¼‰å€¤5-13%å‰Šæ¸›æ¤œè¨¼ï¼‰"""
        self.logger.info(f"Running baseline comparison: MLA vs Standard Attention")
        self.logger.info(f"Config: seq_len={seq_length}, batch_size={batch_size}, precision={precision}")
        
        # DeepSeek R1 (MLA) æ¸¬å®š
        mla_result = self.run_benchmark_single_config(seq_length, batch_size, precision)
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        torch.cuda.empty_cache() if self.device == "cuda" else None
        gc.collect()
        
        # æ¨™æº–Attentionï¼ˆLlama-2ï¼‰æ¸¬å®š
        original_model_name = self.config.model_name
        self.config.model_name = self.config.baseline_model_name
        
        baseline_result = self.run_benchmark_single_config(seq_length, batch_size, precision)
        baseline_result.attention_type = "Standard Attention"
        
        # å…ƒã®ãƒ¢ãƒ‡ãƒ«åå¾©å…ƒ
        self.config.model_name = original_model_name
        
        # åŠ¹ç‡æ”¹å–„ç‡è¨ˆç®—ï¼ˆè«–æ–‡è¨˜è¼‰å€¤æ¤œè¨¼ï¼‰
        kv_reduction_percent = (
            (baseline_result.kv_cache_memory_mb - mla_result.kv_cache_memory_mb) 
            / baseline_result.kv_cache_memory_mb * 100
        )
        
        self.logger.info(f"KV Cache Memory Reduction: {kv_reduction_percent:.2f}%")
        self.logger.info(f"Paper claim validation (5-13%): {'âœ“ PASS' if 5 <= kv_reduction_percent <= 13 else 'âœ— FAIL'}")
        
        return mla_result, baseline_result
    
    def validate_paper_claims(self) -> Dict[str, Any]:
        """è«–æ–‡è¨˜è¼‰å€¤ã®åŒ…æ‹¬çš„æ¤œè¨¼"""
        validation_results = {
            "paper_claim_5_13_percent": {"target_range": (5, 13), "measurements": []},
            "overall_validation": False,
            "detailed_results": []
        }
        
        for seq_length in self.config.sequence_lengths:
            for batch_size in self.config.batch_sizes:
                for precision in self.config.precision_modes:
                    mla_result, baseline_result = self.run_baseline_comparison(
                        seq_length, batch_size, precision
                    )
                    
                    # å‰Šæ¸›ç‡è¨ˆç®—
                    reduction_percent = (
                        (baseline_result.kv_cache_memory_mb - mla_result.kv_cache_memory_mb)
                        / baseline_result.kv_cache_memory_mb * 100
                    )
                    
                    validation_results["paper_claim_5_13_percent"]["measurements"].append({
                        "seq_length": seq_length,
                        "batch_size": batch_size,
                        "precision": precision,
                        "reduction_percent": reduction_percent,
                        "mla_memory_mb": mla_result.kv_cache_memory_mb,
                        "baseline_memory_mb": baseline_result.kv_cache_memory_mb,
                        "validates_claim": 5 <= reduction_percent <= 13
                    })
                    
                    validation_results["detailed_results"].extend([mla_result, baseline_result])
        
        # å…¨ä½“æ¤œè¨¼åˆ¤å®š
        measurements = validation_results["paper_claim_5_13_percent"]["measurements"]
        valid_count = sum(1 for m in measurements if m["validates_claim"])
        validation_results["overall_validation"] = valid_count / len(measurements) >= 0.8  # 80%ä»¥ä¸Šã§åˆæ ¼
        
        return validation_results
    
    def run_benchmark_single_config(self, seq_length: int, batch_size: int, precision: str) -> AttentionBenchmarkResult:
        """å˜ä¸€è¨­å®šã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        self.logger.info(f"Running benchmark: seq_len={seq_length}, batch_size={batch_size}, precision={precision}")
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        model, tokenizer = self.load_model_with_precision(precision)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        input_ids = self.generate_test_sequences(tokenizer, seq_length, batch_size)
        
        # å„ç¨®æ¸¬å®š
        kv_cache_memory = self.measure_kv_cache_memory(model, input_ids)
        attention_time = self.measure_attention_computation_time(model, input_ids)
        throughput = self.measure_throughput(model, tokenizer, input_ids)
        
        # ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024) if self.device == "cuda" else 0
        
        # çµæœä½œæˆ
        result = AttentionBenchmarkResult(
            model_name=self.config.model_name,
            attention_type="MLA" if "deepseek" in self.config.model_name.lower() else "Standard",
            sequence_length=seq_length,
            batch_size=batch_size,
            precision=precision,
            kv_cache_memory_mb=kv_cache_memory,
            attention_computation_time_ms=attention_time,
            peak_memory_usage_mb=peak_memory,
            throughput_tokens_per_sec=throughput,
            measurement_timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del model, tokenizer
        gc.collect()
        if self.device == "cuda":
            torch.cuda.empty_cache()
            
        return result
    
    def run_full_benchmark(self) -> List[AttentionBenchmarkResult]:
        """å…¨è¨­å®šã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        self.logger.info("Starting full MLA efficiency benchmark")
        
        results = []
        
        for precision in self.config.precision_modes:
            for seq_length in self.config.sequence_lengths:
                for batch_size in self.config.batch_sizes:
                    try:
                        result = self.run_benchmark_single_config(seq_length, batch_size, precision)
                        results.append(result)
                        self.results.append(result)
                        
                        self.logger.info(f"Completed: {result.attention_type} - KV Cache: {result.kv_cache_memory_mb:.2f}MB")
                        
                    except Exception as e:
                        self.logger.error(f"Failed benchmark for seq_len={seq_length}, batch_size={batch_size}: {e}")
                        continue
        
        return results
    
    def calculate_efficiency_metrics(self, deepseek_results: List[AttentionBenchmarkResult], 
                                   baseline_results: List[AttentionBenchmarkResult]) -> Dict[str, Any]:
        """åŠ¹ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        metrics = {
            'kv_cache_reduction_percent': [],
            'speed_improvement_percent': [],
            'memory_reduction_percent': [],
            'throughput_improvement_percent': []
        }
        
        for ds_result in deepseek_results:
            # å¯¾å¿œã™ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœã‚’æ¢ç´¢
            baseline_result = None
            for bl_result in baseline_results:
                if (bl_result.sequence_length == ds_result.sequence_length and
                    bl_result.batch_size == ds_result.batch_size and
                    bl_result.precision == ds_result.precision):
                    baseline_result = bl_result
                    break
            
            if baseline_result:
                # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šæ¸›ç‡
                kv_reduction = ((baseline_result.kv_cache_memory_mb - ds_result.kv_cache_memory_mb) / 
                               baseline_result.kv_cache_memory_mb) * 100
                metrics['kv_cache_reduction_percent'].append(kv_reduction)
                
                # é€Ÿåº¦å‘ä¸Š
                speed_improvement = ((baseline_result.attention_computation_time_ms - ds_result.attention_computation_time_ms) / 
                                   baseline_result.attention_computation_time_ms) * 100
                metrics['speed_improvement_percent'].append(speed_improvement)
                
                # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
                memory_reduction = ((baseline_result.peak_memory_usage_mb - ds_result.peak_memory_usage_mb) / 
                                  baseline_result.peak_memory_usage_mb) * 100
                metrics['memory_reduction_percent'].append(memory_reduction)
                
                # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š
                throughput_improvement = ((ds_result.throughput_tokens_per_sec - baseline_result.throughput_tokens_per_sec) / 
                                        baseline_result.throughput_tokens_per_sec) * 100
                metrics['throughput_improvement_percent'].append(throughput_improvement)
        
        # çµ±è¨ˆè¨ˆç®—
        summary_metrics = {}
        for key, values in metrics.items():
            if values:
                summary_metrics[key] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'values': values
                }
        
        return summary_metrics
    
    def save_results(self, filename: Optional[str] = None):
        """çµæœä¿å­˜"""
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"mla_benchmark_results_{timestamp}.json"
        
        output_path = Path(self.config.output_dir) / filename
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # çµæœã‚’JSONå½¢å¼ã§ä¿å­˜
        results_dict = {
            'config': {
                'model_name': self.config.model_name,
                'sequence_lengths': self.config.sequence_lengths,
                'batch_sizes': self.config.batch_sizes,
                'precision_modes': self.config.precision_modes,
                'num_runs': self.config.num_runs
            },
            'results': [
                {
                    'model_name': r.model_name,
                    'attention_type': r.attention_type,
                    'sequence_length': r.sequence_length,
                    'batch_size': r.batch_size,
                    'precision': r.precision,
                    'kv_cache_memory_mb': r.kv_cache_memory_mb,
                    'attention_computation_time_ms': r.attention_computation_time_ms,
                    'peak_memory_usage_mb': r.peak_memory_usage_mb,
                    'throughput_tokens_per_sec': r.throughput_tokens_per_sec,
                    'measurement_timestamp': r.measurement_timestamp
                }
                for r in self.results
            ]
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"Results saved to {output_path}")
        return output_path

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° - è«–æ–‡è¨˜è¼‰å€¤æ¤œè¨¼å«ã‚€"""
    print("=== MLA vs Standard Attention Benchmark (Paper Validation) ===")
    
    # è¨­å®š
    config = MLABenchmarkConfig(
        model_name="deepseek-ai/deepseek-r1-distill-qwen-1.5b",
        baseline_model_name="meta-llama/Llama-2-7b-hf",
        sequence_lengths=[512, 1024, 2048],  # é«˜é€ŸåŒ–ã®ãŸã‚å‰Šæ¸›
        batch_sizes=[1, 2, 4],
        precision_modes=["fp16"],
        num_runs=3,
        warmup_runs=1,
        output_dir="./benchmark_results"
    )
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ 
    measurer = MLAEfficiencyMeasurer(config)
    
    # è«–æ–‡è¨˜è¼‰å€¤æ¤œè¨¼å®Ÿè¡Œï¼ˆæœ€é‡è¦ï¼‰
    print("\nğŸ”¬ Validating Paper Claims: 5-13% KV Cache Memory Reduction")
    validation_results = measurer.validate_paper_claims()
    
    # æ¤œè¨¼çµæœè¡¨ç¤º
    print(f"\nPaper Validation Results:")
    print(f"Target: 5-13% KV cache memory reduction")
    print(f"Overall validation: {'âœ… PASS' if validation_results['overall_validation'] else 'âŒ FAIL'}")
    
    measurements = validation_results["paper_claim_5_13_percent"]["measurements"]
    valid_measurements = [m for m in measurements if m["validates_claim"]]
    
    print(f"Valid measurements: {len(valid_measurements)}/{len(measurements)}")
    
    if measurements:
        reductions = [m["reduction_percent"] for m in measurements]
        print(f"Reduction range: {min(reductions):.2f}% - {max(reductions):.2f}%")
        print(f"Average reduction: {np.mean(reductions):.2f}%")
    
    # çµæœä¿å­˜
    results_file = measurer.save_results("mla_paper_validation_results.json")
    
    # è©³ç´°çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    validation_file = Path(config.output_dir) / "paper_validation_results.json"
    validation_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(validation_file, 'w', encoding='utf-8') as f:
        json.dump(validation_results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nResults saved to:")
    print(f"- Benchmark: {results_file}")
    print(f"- Validation: {validation_file}")
    
    # Opinion.md R-1 å¯¾å¿œå®Œäº†å ±å‘Š
    print(f"\nğŸ“‹ Opinion.md R-1 Status: {'âœ… VALIDATED' if validation_results['overall_validation'] else 'âŒ REQUIRES INVESTIGATION'}")
    
    return validation_results

if __name__ == "__main__":
    main()
