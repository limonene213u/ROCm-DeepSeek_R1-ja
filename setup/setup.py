# setup.py - DeepSeek R1æ—¥æœ¬èªç‰¹åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆRunPod ROCm 6.1.0å¯¾å¿œï¼‰
import os
import sys
import subprocess
import importlib
import importlib.util
import platform
import json
from typing import List, Tuple, Dict, Any


def print_banner() -> None:
    """ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒãƒŠãƒ¼è¡¨ç¤º"""
    print("=" * 65)
    print("  DeepSeek R1 æ—¥æœ¬èªç‰¹åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print("  ROCm 6.1.0 + AMD MI300X æœ€é©åŒ–ç‰ˆ")
    print("=" * 65)


def check_system_requirements() -> Dict[str, Any]:
    """ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ãƒã‚§ãƒƒã‚¯"""
    print("ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ãƒã‚§ãƒƒã‚¯ä¸­...")
    
    system_info = {
        "python_version": sys.version_info,
        "platform": platform.system(),
        "architecture": platform.architecture()[0],
        "cpu_count": os.cpu_count(),
        "requirements_met": True,
        "warnings": []
    }
    
    # Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
    if system_info["python_version"] < (3, 10):
        print(f"[ERROR] Python 3.10+ å¿…è¦ï¼ˆç¾åœ¨: {sys.version}ï¼‰")
        system_info["requirements_met"] = False
    else:
        print(f"[OK] Python: {sys.version}")
    
    # ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
    try:
        import psutil
        memory_gb = psutil.virtual_memory().total / 1e9
        print(f"[INFO] ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª: {memory_gb:.1f}GB")
        
        if memory_gb < 16:
            system_info["warnings"].append(
                "16GBä»¥ä¸Šã®ãƒ¡ãƒ¢ãƒªã‚’æ¨å¥¨ï¼ˆæ—¥æœ¬èªè¨€èªãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨ï¼‰"
            )
    except ImportError:
        system_info["warnings"].append("psutilæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ä¸å¯")
    
    return system_info


def detect_gpu_environment() -> Tuple[bool, str, Dict[str, Any]]:
    """GPUç’°å¢ƒè‡ªå‹•æ¤œå‡º"""
    print("GPUç’°å¢ƒæ¤œå‡ºä¸­...")
    
    gpu_info = {
        "has_gpu": False,
        "gpu_type": "None",
        "gpu_count": 0,
        "total_vram_gb": 0,
        "recommended_model": "deepseek-r1-distill-qwen-1.5b"
    }
    
    try:
        import torch
        
        if torch.cuda.is_available():
            gpu_info.update({
                "has_gpu": True,
                "gpu_type": "CUDA",
                "gpu_count": torch.cuda.device_count(),
            })
            
            # GPUè©³ç´°æƒ…å ±
            for i in range(gpu_info["gpu_count"]):
                device_name = torch.cuda.get_device_name(i)
                props = torch.cuda.get_device_properties(i)
                vram_gb = props.total_memory / 1e9
                gpu_info["total_vram_gb"] += vram_gb
                
                print(f"[INFO] GPU {i}: {device_name} ({vram_gb:.1f}GB)")
            
            # æ¨å¥¨ãƒ¢ãƒ‡ãƒ«åˆ¤å®šï¼ˆDeepSeek R1ç”¨ï¼‰
            if gpu_info["total_vram_gb"] >= 64:
                gpu_info["recommended_model"] = "deepseek-r1-distill-qwen-7b"
                print("[OK] 7Bãƒ¢ãƒ‡ãƒ«å­¦ç¿’å¯èƒ½")
            elif gpu_info["total_vram_gb"] >= 16:
                gpu_info["recommended_model"] = "deepseek-r1-distill-qwen-1.5b"
                print("[OK] 1.5Bãƒ¢ãƒ‡ãƒ«å­¦ç¿’å¯èƒ½")
            else:
                print("[WARNING] GPU VRAMä¸è¶³ - è¨­å®šèª¿æ•´ãŒå¿…è¦")
        
        # ROCmç’°å¢ƒãƒã‚§ãƒƒã‚¯ï¼ˆRunPod ROCm 6.1.0å¯¾å¿œï¼‰
        elif hasattr(torch, 'cuda') and torch.cuda.is_available():
            gpu_info.update({
                "has_gpu": True,
                "gpu_type": "ROCm",
                "gpu_count": torch.cuda.device_count(),
            })
            
            # ROCm 6.1.0 ç’°å¢ƒè©³ç´°æƒ…å ±
            try:
                for i in range(gpu_info["gpu_count"]):
                    device_name = torch.cuda.get_device_name(i)
                    props = torch.cuda.get_device_properties(i)
                    vram_gb = props.total_memory / 1e9
                    gpu_info["total_vram_gb"] += vram_gb
                    print(f"[INFO] ROCm GPU {i}: {device_name} ({vram_gb:.1f}GB)")
                
                # MI300ã‚·ãƒªãƒ¼ã‚ºã®ç‰¹åˆ¥æ‰±ã„
                if "MI300" in str(torch.cuda.get_device_name(0)):
                    gpu_info["recommended_model"] = "deepseek-r1-distill-qwen-7b"
                    print("[OK] MI300ã‚·ãƒªãƒ¼ã‚ºæ¤œå‡º - 7Bãƒ¢ãƒ‡ãƒ«æ¨å¥¨")
                else:
                    gpu_info["recommended_model"] = "deepseek-r1-distill-qwen-7b"
                    
            except Exception:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                gpu_info["recommended_model"] = "deepseek-r1-distill-qwen-1.5b"
                
            print(f"[INFO] ROCmç’°å¢ƒæ¤œå‡º (RunPod ROCm 6.1.0)")
        
        # ROCmç’°å¢ƒã®å ´åˆã€torch.cuda.is_available()ãŒTrueã§ã‚‚GPUç¨®åˆ¥ã‚’ROCmã«å¤‰æ›´
        if "gfx" in str(torch.cuda.get_device_properties(0).name).lower():
            if gpu_info["gpu_type"] != "ROCm":
                print("[INFO] CUDA/ROCmæ··åœ¨æ¤œå‡º - ROCmç’°å¢ƒã¨ã—ã¦æ‰±ã„ã¾ã™")
                gpu_info["gpu_type"] = "ROCm"
        
        
        else:
            print("[WARNING] GPUæœªæ¤œå‡º - CPUå­¦ç¿’ï¼ˆéå¸¸ã«é…ã„ï¼‰")
            
    except ImportError:
        print("[WARNING] PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
        return False, "Unknown", gpu_info
    
    return gpu_info["has_gpu"], gpu_info["gpu_type"], gpu_info


def install_pytorch(gpu_type: str) -> bool:
    """GPUç’°å¢ƒã«å¿œã˜ãŸPyTorchè‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    print(f"PyTorch ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­ï¼ˆ{gpu_type}ç’°å¢ƒç”¨ï¼‰...")
    
    if gpu_type == "ROCm":
        # ROCmç”¨PyTorchï¼ˆRunPod ROCm 6.1.0ç’°å¢ƒå¯¾å¿œï¼‰
        # æ—¢å­˜PyTorchã‚’ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        print("æ—¢å­˜PyTorchã‚’ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
        try:
            subprocess.run([
                sys.executable, "-m", "pip", "uninstall", 
                "torch", "torchvision", "torchaudio", "-y"
            ], check=False, capture_output=True)  # ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–
        except:
            pass
        
        # ROCmç‰ˆPyTorchã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        cmd = [
            sys.executable, "-m", "pip", "install", 
            "torch", "torchvision", "torchaudio", 
            "--index-url", "https://download.pytorch.org/whl/rocm6.1"
        ]
        print("ROCm 6.1.0ç‰ˆPyTorchã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
    else:
        # CUDA/CPUç”¨PyTorch
        cmd = [
            sys.executable, "-m", "pip", "install", 
            "torch", "torchvision", "torchaudio"
        ]
        print("CUDAç‰ˆPyTorchã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
    
    try:
        subprocess.run(cmd, check=True, capture_output=True)
        print("[OK] PyTorch ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
        
        # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
        try:
            import torch
            print(f"[OK] PyTorch ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.__version__}")
            if torch.cuda.is_available():
                print(f"[OK] CUDA ãƒ‡ãƒã‚¤ã‚¹åˆ©ç”¨å¯èƒ½")
            else:
                print("[INFO] CPUç‰ˆPyTorch")
        except ImportError:
            print("[WARNING] PyTorchã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèªå¤±æ•—")
            
        return True
    except subprocess.CalledProcessError as e:
        print(f"[ERROR] PyTorch ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—: {e}")
        return False


def install_requirements(gpu_type: str) -> bool:
    """requirements.txt ã‹ã‚‰ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    print("ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
    
    # bitsandbytes ã¯ ROCmç’°å¢ƒã§ã¯é™¤å¤–
    excluded_packages = []
    if gpu_type == "ROCm":
        excluded_packages = ["bitsandbytes"]
        print("ğŸ”¥ ROCmç’°å¢ƒï¼šbitsandbytesã‚’ã‚¹ã‚­ãƒƒãƒ—")
    
    try:
        # requirements.txt ã‚’ä¸€æ™‚çš„ã«ä¿®æ­£ï¼ˆROCmç’°å¢ƒç”¨ï¼‰
        with open("requirements.txt", "r") as f:
            requirements = f.readlines()
        
        filtered_requirements = []
        for line in requirements:
            skip = False
            for excluded in excluded_packages:
                if excluded in line and not line.strip().startswith("#"):
                    skip = True
                    break
            if not skip:
                filtered_requirements.append(line)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        temp_req_file = "requirements_filtered.txt"
        with open(temp_req_file, "w") as f:
            f.writelines(filtered_requirements)
        
        # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Ÿè¡Œ
        cmd = [sys.executable, "-m", "pip", "install", "-r", temp_req_file]
        subprocess.run(cmd, check=True)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        os.remove(temp_req_file)
        
        print("[OK] ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"[ERROR] ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—: {e}")
        return False
    except Exception as e:
        print(f"[ERROR] ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def verify_installation() -> Dict[str, bool]:
    """ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¤œè¨¼"""
    print("ğŸ” ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¤œè¨¼ä¸­...")
    
    verification_results = {}
    
    # å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç¢ºèª
    required_packages = [
        "torch", "transformers", "peft", "datasets", 
        "accelerate", "tqdm", "numpy"
    ]
    
    for package in required_packages:
        try:
            importlib.import_module(package)
            print(f"[OK] {package}")
            verification_results[package] = True
        except ImportError:
            print(f"[ERROR] {package}")
            verification_results[package] = False
    
    # ç‰¹åˆ¥ãªãƒã‚§ãƒƒã‚¯
    try:
        import torch
        if torch.cuda.is_available():
            print("[OK] GPUåŠ é€Ÿåˆ©ç”¨å¯èƒ½")
            verification_results["gpu_acceleration"] = True
        else:
            print("[WARNING] GPUåŠ é€Ÿåˆ©ç”¨ä¸å¯")
            verification_results["gpu_acceleration"] = False
    except:
        verification_results["gpu_acceleration"] = False
    
    return verification_results


def create_project_structure() -> None:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ä½œæˆ"""
    print("ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ä½œæˆä¸­...")
    
    directories = [
        "data",
        "models", 
        "logs",
        "results",
        "deployment",
        "tests"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"ğŸ“ {directory}/")


def create_sample_files() -> None:
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    print("ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­...")
    
    # ã‚µãƒ³ãƒ—ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    sample_config = {
        "character_name": "akatsuki",
        "model_name": "nekomata-14b",
        "training_config": {
            "batch_size": 4,
            "learning_rate": 1e-4,
            "num_epochs": 5
        }
    }
    
    with open("config_sample.json", "w", encoding="utf-8") as f:
        json.dump(sample_config, f, indent=2, ensure_ascii=False)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    sample_dataset = [
        {
            "messages": [
                {
                    "role": "system",
                    "content": "ã‚ãªãŸã¯DeepSeek R1ã®æ—¥æœ¬èªç‰¹åŒ–ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
                },
                {
                    "role": "user",
                    "content": "ã“ã‚“ã«ã¡ã¯"
                },
                {
                    "role": "assistant",
                    "content": "ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯DeepSeek R1ã®æ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
                }
            ]
        }
    ]
    
    with open("data/sample_dataset.jsonl", "w", encoding="utf-8") as f:
        for item in sample_dataset:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print("ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†")


def generate_quick_start_guide(gpu_info: Dict[str, Any]) -> None:
    """ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰ç”Ÿæˆï¼ˆRunPod ROCm 6.1.0å¯¾å¿œï¼‰"""
    # RunPodç’°å¢ƒã®æ¤œå‡º
    runpod_env = ""
    if gpu_info.get('gpu_type', '') == 'ROCm':
        runpod_env = "- ç’°å¢ƒ: RunPod PyTorch 2.4.0-py3.10-rocm6.1.0-ubuntu22.04\n"
    
    guide_content = f"""# DeepSeek R1 æ—¥æœ¬èªç‰¹åŒ–å­¦ç¿’ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰

## ğŸ¯ æ¤œå‡ºã•ã‚ŒãŸç’°å¢ƒ
- GPU: {gpu_info.get('gpu_type', 'None')}
- VRAM: {gpu_info.get('total_vram_gb', 0):.1f}GB
- æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {gpu_info.get('recommended_model', 'nekomata-7b')}
{runpod_env}

## ğŸ“‹ åŸºæœ¬å®Ÿè¡Œæ‰‹é †

### 1. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆæ¨å¥¨ï¼šæœ€åˆã«å®Ÿè¡Œï¼‰
```bash
# ä¾å­˜é–¢ä¿‚ç¢ºèª
python main.py --check-only

# 7Bãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆï¼ˆç´„30åˆ†ã€å®‰ä¾¡ï¼‰
python main.py --preset akatsuki --model nekomata-7b
```

### 2. æœ¬æ ¼å­¦ç¿’
```bash
# æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã§æœ¬æ ¼å­¦ç¿’
python main.py --preset akatsuki --model {gpu_info.get('recommended_model', 'nekomata-7b')}
```

### 3. ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
```bash
# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†å¾Œ
python main.py --config config_sample.json
```

## ğŸ“ RunPod ROCm 6.1.0 ç’°å¢ƒã§ã®æ³¨æ„äº‹é …
- PyTorch 2.4.0 + ROCm 6.1.0 ã®æœ€æ–°ç’°å¢ƒã«æœ€é©åŒ–æ¸ˆã¿
- MI300ã‚·ãƒªãƒ¼ã‚º GPU ã§æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®
- åˆå›å®Ÿè¡Œæ™‚ã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™
- GPU VRAMä¸è¶³ã®å ´åˆã¯ batch_size ã‚’èª¿æ•´ã—ã¦ãã ã•ã„
- å­¦ç¿’ãƒ­ã‚°ã¯ logs/ ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã™

## ğŸ†˜ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- CUDA Out of Memory â†’ batch_size ã‚’æ¸›ã‚‰ã™
- ModuleNotFoundError â†’ pip install -r requirements.txt
- GPUæœªæ¤œå‡º â†’ ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèª
- ROCmé–¢é€£ã‚¨ãƒ©ãƒ¼ â†’ ROCm 6.1.0 ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèª

è©³ç´°ã¯Documentation/ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚¬ã‚¤ãƒ‰ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
"""
    
    with open("QUICKSTART.md", "w", encoding="utf-8") as f:
        f.write(guide_content)
    
    print("ğŸ“ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰ç”Ÿæˆå®Œäº†: QUICKSTART.md")


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        print_banner()
        
        # ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ãƒã‚§ãƒƒã‚¯
        system_info = check_system_requirements()
        if not system_info["requirements_met"]:
            print("[ERROR] ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“")
            sys.exit(1)
        
        # è­¦å‘Šè¡¨ç¤º
        for warning in system_info["warnings"]:
            print(f"[WARNING] {warning}")
        
        # GPUç’°å¢ƒæ¤œå‡º
        has_gpu, gpu_type, gpu_info = detect_gpu_environment()
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ä½œæˆ
        create_project_structure()
        
        # PyTorchã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèªã¨å¼·åˆ¶ROCmå¯¾å¿œ
        force_rocm = False
        try:
            import torch
            current_version = torch.__version__
            is_rocm = "rocm" in current_version.lower()
            
            print(f"[OK] PyTorchæ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿: {current_version}")
            
            # CUDAç‰ˆãŒå…¥ã£ã¦ã„ã‚‹å ´åˆã€ROCmç’°å¢ƒã§ã¯å¼·åˆ¶çš„ã«åˆ‡ã‚Šæ›¿ãˆ
            if not is_rocm and gpu_type == "ROCm":
                print("[WARNING] CUDAç‰ˆPyTorchãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ROCmç‰ˆã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
                force_rocm = True
            elif is_rocm:
                print(f"[OK] ROCmç‰ˆPyTorchç¢ºèªæ¸ˆã¿")
                
        except ImportError:
            print("[WARNING] PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
            force_rocm = (gpu_type == "ROCm")
        
        # PyTorchã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«/å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        if force_rocm or not importlib.util.find_spec("torch"):
            if not install_pytorch(gpu_type):
                print("[ERROR] PyTorchã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—")
                sys.exit(1)
        
        # ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        if not install_requirements(gpu_type):
            print("[ERROR] ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—")
            sys.exit(1)
        
        # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¤œè¨¼
        verification_results = verify_installation()
        failed_packages = [pkg for pkg, success in verification_results.items() if not success]
        
        if failed_packages:
            print(f"[WARNING] ä¸€éƒ¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—: {failed_packages}")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        create_sample_files()
        
        # ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰ç”Ÿæˆ
        generate_quick_start_guide(gpu_info)
        
        # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print("\n" + "=" * 50)
        print("[OK] ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼")
        print("QUICKSTART.md ã‚’ç¢ºèªã—ã¦å­¦ç¿’ã‚’é–‹å§‹ã—ã¦ãã ã•ã„")
        print("=" * 50)
        
    except KeyboardInterrupt:
        print("\n[ERROR] ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"[ERROR] äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()